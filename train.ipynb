{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤변수 통제 함수\n",
    "def seed_everything(seed):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 커스텀 데이터 세트 정의\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, x):\n",
    "    self.x = x\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    x = self.x[index]\n",
    "    return torch.Tensor(x)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정상 데이터 사전 훈련을 위한 AutoEncoder 모델 정의\n",
    "class AE(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "      # Deep SVDD에서 bias를 True로 두면 trival solution이 나타나기 때문에 bias를 없애주어야 함\n",
    "      # AE + LOF는 bias True로 두어도 괜찮음\n",
    "      nn.Linear(7, 32, bias=False),\n",
    "      nn.BatchNorm1d(32, affine=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 16, bias=False),\n",
    "      nn.BatchNorm1d(16, affine=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(16, 8, bias=False),\n",
    "      nn.BatchNorm1d(8, affine=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(8, 3, bias=False) # 3차원으로 압축\n",
    "    )\n",
    "    \n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Linear(3, 8, bias=False),\n",
    "      nn.BatchNorm1d(8, affine=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(8, 16, bias=False),\n",
    "      nn.BatchNorm1d(16, affine=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(16, 32, bias=False),\n",
    "      nn.BatchNorm1d(32, affine=False),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 7, bias=False)\n",
    "    )\n",
    "  def encode(self, x):\n",
    "    return self.encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    return self.decoder(x)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.encode(x)\n",
    "    x = self.decode(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련을 위한 Trainer 클래스 정의\n",
    "class Trainer:\n",
    "  def __init__(self):\n",
    "    self.epochs = 70\n",
    "    self.lr = 0.001\n",
    "    self.seed = 911\n",
    "    self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "  def setup(self, train_data_path):\n",
    "    seed_everything(self.seed) # 랜덤시드 설정\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    # 훈련시 train_df에 type열이 있으면 아래 코드의 주석을 해제해주세요\n",
    "    train_df.drop(columns=[\"type\"], inplace=True)\n",
    "    scaler = preprocessing.StandardScaler() # 숫자 데이터는 표준화 해주는 것이 성능 향상에 도움이 됨\n",
    "    scaled_train = scaler.fit_transform(train_df)\n",
    "    \n",
    "    train_dataset = CustomDataset(x=scaled_train)\n",
    "    self.train_dataloader = DataLoader(\n",
    "      dataset=train_dataset,\n",
    "      batch_size=32,\n",
    "      shuffle=True,\n",
    "      num_workers=0\n",
    "    )\n",
    "    \n",
    "  def _weights_init_normal(self, m):\n",
    "    # 파라미터 랜덤 초기화\n",
    "    if isinstance(m, nn.Linear):\n",
    "      m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "      if m.bias is not None:\n",
    "        m.bias.data.zero_()\n",
    "  \n",
    "  def train(self):\n",
    "    self.model = AE().to(self.device)\n",
    "    self.model.apply(self._weights_init_normal)\n",
    "    optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "      optimizer=optimizer,\n",
    "      mode=\"min\",\n",
    "      factor=0.1,\n",
    "      patience=10\n",
    "    )\n",
    "    \n",
    "    self.model.train()\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "      total_loss = 0\n",
    "      for x in self.train_dataloader:\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = self.model(x)\n",
    "        \n",
    "        recon_loss = torch.mean(torch.sum((output - x) ** 2, dim=tuple(range(1, output.dim()))))\n",
    "        recon_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += recon_loss.item()\n",
    "\n",
    "      total_loss = total_loss/len(self.train_dataloader)\n",
    "      scheduler.step(total_loss)\n",
    "      print(F\"EPOCH: {epoch} | Loss: {total_loss}\")\n",
    "\n",
    "    torch.save({\"param\": self.model.state_dict()}, f\"./AE_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 | Loss: 3.010600622598227\n",
      "EPOCH: 2 | Loss: 1.336807677788394\n",
      "EPOCH: 3 | Loss: 1.0731953678580073\n",
      "EPOCH: 4 | Loss: 0.8932532751327985\n",
      "EPOCH: 5 | Loss: 0.8856466234504402\n",
      "EPOCH: 6 | Loss: 0.7840184410671135\n",
      "EPOCH: 7 | Loss: 0.6592780877049867\n",
      "EPOCH: 8 | Loss: 0.49815415107197575\n",
      "EPOCH: 9 | Loss: 0.5415858570244405\n",
      "EPOCH: 10 | Loss: 0.42986634663947215\n",
      "EPOCH: 11 | Loss: 0.4481877069194595\n",
      "EPOCH: 12 | Loss: 0.38203221336974724\n",
      "EPOCH: 13 | Loss: 0.4674478147130508\n",
      "EPOCH: 14 | Loss: 0.39256715150429056\n",
      "EPOCH: 15 | Loss: 0.3471336748402614\n",
      "EPOCH: 16 | Loss: 0.30872259782506273\n",
      "EPOCH: 17 | Loss: 0.28469432803330486\n",
      "EPOCH: 18 | Loss: 0.3036712133748965\n",
      "EPOCH: 19 | Loss: 0.3233362057959879\n",
      "EPOCH: 20 | Loss: 0.25578938898715103\n",
      "EPOCH: 21 | Loss: 0.33158961818976834\n",
      "EPOCH: 22 | Loss: 0.27197882840288923\n",
      "EPOCH: 23 | Loss: 0.3147136362922656\n",
      "EPOCH: 24 | Loss: 0.2725082644587987\n",
      "EPOCH: 25 | Loss: 0.24885935655661992\n",
      "EPOCH: 26 | Loss: 0.33231717115872866\n",
      "EPOCH: 27 | Loss: 0.2983536461440774\n",
      "EPOCH: 28 | Loss: 0.26270774026195726\n",
      "EPOCH: 29 | Loss: 0.28284248361339814\n",
      "EPOCH: 30 | Loss: 0.22990638738522282\n",
      "EPOCH: 31 | Loss: 0.270164000668696\n",
      "EPOCH: 32 | Loss: 0.26935633276770643\n",
      "EPOCH: 33 | Loss: 0.22928386271096668\n",
      "EPOCH: 34 | Loss: 0.27245783583297356\n",
      "EPOCH: 35 | Loss: 0.28959855345356\n",
      "EPOCH: 36 | Loss: 0.23517882814268012\n",
      "EPOCH: 37 | Loss: 0.26402406186445965\n",
      "EPOCH: 38 | Loss: 0.23967420251725555\n",
      "EPOCH: 39 | Loss: 0.24313668190658866\n",
      "EPOCH: 40 | Loss: 0.2247609014247919\n",
      "EPOCH: 41 | Loss: 0.21213396653145938\n",
      "EPOCH: 42 | Loss: 0.23271733377273982\n",
      "EPOCH: 43 | Loss: 0.22413677765758006\n",
      "EPOCH: 44 | Loss: 0.21025314617466617\n",
      "EPOCH: 45 | Loss: 0.23826967852262707\n",
      "EPOCH: 46 | Loss: 0.22465732858165519\n",
      "EPOCH: 47 | Loss: 0.20829154478458614\n",
      "EPOCH: 48 | Loss: 0.2533655157433702\n",
      "EPOCH: 49 | Loss: 0.19092380569933295\n",
      "EPOCH: 50 | Loss: 0.19564968662021995\n",
      "EPOCH: 51 | Loss: 0.20365218218270834\n",
      "EPOCH: 52 | Loss: 0.1975052503554465\n",
      "EPOCH: 53 | Loss: 0.23861871891981595\n",
      "EPOCH: 54 | Loss: 0.24973259034094872\n",
      "EPOCH: 55 | Loss: 0.2008633648904113\n",
      "EPOCH: 56 | Loss: 0.2462131247504965\n",
      "EPOCH: 57 | Loss: 0.2525704490480485\n",
      "EPOCH: 58 | Loss: 0.19021603808581056\n",
      "EPOCH: 59 | Loss: 0.20908864645601868\n",
      "EPOCH: 60 | Loss: 0.20222560316324234\n",
      "EPOCH: 61 | Loss: 0.20417786643586375\n",
      "EPOCH: 62 | Loss: 0.21948228933691205\n",
      "EPOCH: 63 | Loss: 0.2343059909808171\n",
      "EPOCH: 64 | Loss: 0.200218873267824\n",
      "EPOCH: 65 | Loss: 0.20196353353850252\n",
      "EPOCH: 66 | Loss: 0.2294103204478304\n",
      "EPOCH: 67 | Loss: 0.1924844802925726\n",
      "EPOCH: 68 | Loss: 0.21964626091641265\n",
      "EPOCH: 69 | Loss: 0.178568201841085\n",
      "EPOCH: 70 | Loss: 0.20571479036823495\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.setup(train_data_path=\"./dataset/train_data.csv\")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aifactory-cLCMmIYn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
